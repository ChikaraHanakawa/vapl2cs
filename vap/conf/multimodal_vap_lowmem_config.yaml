# Multimodal VAP Low Memory Configuration
# Modified from original config to reduce memory usage

datamodule:
  _target_: vap.data.multimodal_datamodule.MultimodalVAPDataModule
  train_path: "data/test/sliding_window_dset_train_with_images.csv"
  val_path: "data/test/sliding_window_dset_val_with_images.csv"
  test_path: "data/test/test_dset_event_with_images.csv"
  horizon: 2
  sample_rate: 16000
  frame_hz: 50
  video_fps: 30
  target_image_size: [112, 112]  # Reduced image size (1/4 of original memory)
  use_images: true
  batch_size: 1  # Reduced batch size from 4 to 1
  num_workers: 2  # Reduced num_workers
  pin_memory: true
  prefetch_factor: 2  # Reduced prefetch factor
  eval_mode: false

module:
  _target_: vap.modules.multimodal_lightning_module.MultimodalVAPModule
  use_images: true
  use_visual_masking: true
  visual_masking_prob: 0.3
  model:
    _target_: vap.modules.MultimodalVAP.MultimodalVAP
    bin_times: [0.2, 0.4, 0.6, 0.8]
    frame_hz: 50
    video_fps: 30
    use_visual: true
    audio_encoder:
      _target_: vap.modules.encoder.EncoderCPC
      load_pretrained: true 
      freeze: true
    visual_encoder:
      _target_: vap.modules.encoder_l2cs.L2CSEncoder
      load_pretrained: true
      freeze: true
    transformer:
      _target_: vap.modules.multimodal_transformer.MultimodalTransformer
      dim: 128  # Reduced from 256
      num_layers: 2  # Reduced from 3
      num_heads: 2  # Reduced from 4
      dff_k: 2  # Reduced from 4
      dropout: 0.1
  optim_fn:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 3.0e-4
    betas: [0.9, 0.999]
    weight_decay: 0.001
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.5
    patience: 2
  val_metric:
    _target_: vap.metrics.VAPMetric
    threshold: 0.5
    event_config:
      min_context_time: 3
      metric_time: 0.2
      metric_pad_time: 0.05
      max_time: 20
      frame_hz: 50
      equal_hold_shift: True
      prediction_region_time: 0.5
      sh_pre_cond_time: 1.0
      sh_post_cond_time: 1.0
      sh_prediction_region_on_active: True
      bc_pre_cond_time: 1.0
      bc_post_cond_time: 1.0
      bc_max_duration: 1.0
      bc_negative_pad_left_time: 1.0
      bc_negative_pad_right_time: 2.0
      long_onset_region_time: 0.2
      long_onset_condition_time: 1.0

trainer:
  _target_: lightning.pytorch.Trainer
  strategy: ddp
  accelerator: gpu
  devices: 1
  default_root_dir: null
  max_epochs: 20
  limit_train_batches: 1.0
  val_check_interval: 0.5
  fast_dev_run: false
  gradient_clip_val: 5.0
  precision: 16  # Added 16-bit precision to save memory
  accumulate_grad_batches: 4  # Accumulate gradients to simulate larger batch
  logger:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    save_dir: runs_multimodal
    project: MultimodalVAP
    name: MultimodalVAP_LowMem
    log_model: false
  callbacks:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val_loss
    mode: min
    save_top_k: 1
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val_loss
    mode: min
    patience: 5
  - _target_: vap.callbacks.VADMaskCallback
    probability: 0.5
    sample_rate: 16000
    frame_hz: 50
    on_train: true
    on_val: false
    on_test: false
  - _target_: vap.callbacks.FlipChannelCallback
    probability: 0.5
    on_train: true
    on_val: false
    on_test: false
